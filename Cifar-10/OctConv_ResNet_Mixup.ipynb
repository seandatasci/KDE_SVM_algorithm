{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OctConv ResNet Mixup.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4NRhkU-KSm1",
        "colab_type": "text"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLzUL5CAKKAy",
        "colab_type": "code",
        "outputId": "ca753b39-4b90-47cc-c424-5f861f6709ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!git clone https://github.com/koshian2/OctConv-TFKeras\n",
        "!mv OctConv-TFKeras/*.py ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'OctConv-TFKeras' already exists and is not an empty directory.\n",
            "mv: cannot stat 'OctConv-TFKeras/*.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZtOWg-ZKqw7",
        "colab_type": "text"
      },
      "source": [
        "# Train OctConv Wide ResNet\n",
        "* alpha = 0 -> normal wide res-net\n",
        "* alpha > 0 -> OctConv wide res-net\n",
        "\n",
        "~ 2 hours for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9XxdyWpy7cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from oct_conv2d import OctConv2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def _create_normal_residual_block(inputs, ch, N):\n",
        "    # adujust channels\n",
        "    x = layers.Conv2D(ch, 3, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"elu\")(x)\n",
        "    # Conv with skip connections\n",
        "    for i in range(N-1):\n",
        "        skip = x\n",
        "        x = layers.Conv2D(ch, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"elu\")(x)\n",
        "        x = layers.Conv2D(ch, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"elu\")(x)\n",
        "        x = layers.Add()([x, skip])\n",
        "    return x\n",
        "\n",
        "def _create_octconv_residual_block(inputs, ch, N, alpha):\n",
        "    # adjust channels\n",
        "    high, low = OctConv2D(filters=ch, alpha=alpha)(inputs)\n",
        "    high = layers.BatchNormalization()(high)\n",
        "    high = layers.Activation(\"elu\")(high)\n",
        "    low = layers.BatchNormalization()(low)\n",
        "    low = layers.Activation(\"elu\")(low)\n",
        "    # OctConv with skip connections\n",
        "    for i in range(N-1):\n",
        "        skip_high, skip_low = [high, low]\n",
        "\n",
        "        high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])\n",
        "        high = layers.BatchNormalization()(high)\n",
        "        high = layers.Activation(\"elu\")(high)\n",
        "        low = layers.BatchNormalization()(low)\n",
        "        low = layers.Activation(\"elu\")(low)\n",
        "\n",
        "        high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])\n",
        "        high = layers.BatchNormalization()(high)\n",
        "        high = layers.Activation(\"elu\")(high)\n",
        "        low = layers.BatchNormalization()(low)\n",
        "        low = layers.Activation(\"elu\")(low)\n",
        "\n",
        "        high = layers.Add()([high, skip_high])\n",
        "        low = layers.Add()([low, skip_low])\n",
        "    return [high, low]\n",
        "\n",
        "def create_normal_wide_resnet(N=4, k=10):\n",
        "    \"\"\"\n",
        "    Create vanilla conv Wide ResNet (N=4, k=10)\n",
        "    \"\"\"\n",
        "    # input\n",
        "    input = layers.Input((32,32,3))\n",
        "\n",
        "    # 1st block\n",
        "    x = _create_normal_residual_block(input, 16*k, N)\n",
        "    # 2nd block\n",
        "    x = layers.AveragePooling2D(2)(x)\n",
        "    x = _create_normal_residual_block(x, 32*k, N)\n",
        "    # 3rd block\n",
        "    x = layers.AveragePooling2D(2)(x)\n",
        "    x = _create_normal_residual_block(x, 64*k, N)\n",
        "    # FC\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(input, x)\n",
        "    return model\n",
        "\n",
        "def create_octconv_wide_resnet(alpha, N=4, k=10):\n",
        "    \"\"\"\n",
        "    Create OctConv Wide ResNet(N=4, k=10)\n",
        "    \"\"\"\n",
        "    # Input\n",
        "    input = layers.Input((32,32,3))\n",
        "    # downsampling for lower\n",
        "    low = layers.AveragePooling2D(2)(input)\n",
        "\n",
        "    # 1st block\n",
        "    high, low = _create_octconv_residual_block([input, low], 16*k, N, alpha)\n",
        "    # 2nd block\n",
        "    high = layers.AveragePooling2D(2)(high)\n",
        "    low = layers.AveragePooling2D(2)(low)\n",
        "    high, low = _create_octconv_residual_block([high, low], 32*k, N, alpha)\n",
        "    # 3rd block\n",
        "    high = layers.AveragePooling2D(2)(high)\n",
        "    low = layers.AveragePooling2D(2)(low)\n",
        "    high, low = _create_octconv_residual_block([high, low], 64*k, N, alpha)\n",
        "    # concat\n",
        "    high = layers.AveragePooling2D(2)(high)\n",
        "    x = layers.Concatenate()([high, low])\n",
        "    x = layers.Conv2D(64*k, 1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"elu\")(x)\n",
        "    # FC\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(input, x)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFoDyGqEziG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "class OctConv2D(layers.Layer):\n",
        "    def __init__(self, filters, alpha, kernel_size=(3,3), strides=(1,1), \n",
        "                    padding=\"same\", kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=l2(1e-4), kernel_constraint=None,\n",
        "                    **kwargs):\n",
        "        \"\"\"\n",
        "        OctConv2D : Octave Convolution for image( rank 4 tensors)\n",
        "        filters: # output channels for low + high\n",
        "        alpha: Low channel ratio (alpha=0 -> High only, alpha=1 -> Low only)\n",
        "        kernel_size : 3x3 by default, padding : same by default\n",
        "        \"\"\"\n",
        "        assert alpha >= 0 and alpha <= 1\n",
        "        assert filters > 0 and isinstance(filters, int)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.filters = filters\n",
        "        # optional values\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        # -> Low Channels \n",
        "        self.low_channels = int(self.filters * self.alpha)\n",
        "        # -> High Channles\n",
        "        self.high_channels = self.filters - self.low_channels\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        assert len(input_shape[0]) == 4 and len(input_shape[1]) == 4\n",
        "        # Assertion for high inputs\n",
        "        assert input_shape[0][1] // 2 >= self.kernel_size[0]\n",
        "        assert input_shape[0][2] // 2 >= self.kernel_size[1]\n",
        "        # Assertion for low inputs\n",
        "        assert input_shape[0][1] // input_shape[1][1] == 2\n",
        "        assert input_shape[0][2] // input_shape[1][2] == 2\n",
        "        # channels last for TensorFlow\n",
        "        assert K.image_data_format() == \"channels_last\"\n",
        "        # input channels\n",
        "        high_in = int(input_shape[0][3])\n",
        "        low_in = int(input_shape[1][3])\n",
        "\n",
        "        # High -> High\n",
        "        self.high_to_high_kernel = self.add_weight(name=\"high_to_high_kernel\", \n",
        "                                    shape=(*self.kernel_size, high_in, self.high_channels),\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        # High -> Low\n",
        "        self.high_to_low_kernel  = self.add_weight(name=\"high_to_low_kernel\", \n",
        "                                    shape=(*self.kernel_size, high_in, self.low_channels),\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        # Low -> High\n",
        "        self.low_to_high_kernel  = self.add_weight(name=\"low_to_high_kernel\", \n",
        "                                    shape=(*self.kernel_size, low_in, self.high_channels),\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        # Low -> Low\n",
        "        self.low_to_low_kernel   = self.add_weight(name=\"low_to_low_kernel\", \n",
        "                                    shape=(*self.kernel_size, low_in, self.low_channels),\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Input = [X^H, X^L]\n",
        "        assert len(inputs) == 2\n",
        "        high_input, low_input = inputs\n",
        "        # High -> High conv\n",
        "        high_to_high = K.conv2d(high_input, self.high_to_high_kernel,\n",
        "                                strides=self.strides, padding=self.padding,\n",
        "                                data_format=\"channels_last\")\n",
        "        # High -> Low conv\n",
        "        high_to_low  = K.pool2d(high_input, (2,2), strides=(2,2), pool_mode=\"avg\")\n",
        "        high_to_low  = K.conv2d(high_to_low, self.high_to_low_kernel,\n",
        "                                strides=self.strides, padding=self.padding,\n",
        "                                data_format=\"channels_last\")\n",
        "        # Low -> High conv\n",
        "        low_to_high  = K.conv2d(low_input, self.low_to_high_kernel,\n",
        "                                strides=self.strides, padding=self.padding,\n",
        "                                data_format=\"channels_last\")\n",
        "        low_to_high = K.repeat_elements(low_to_high, 2, axis=1) # Nearest Neighbor Upsampling\n",
        "        low_to_high = K.repeat_elements(low_to_high, 2, axis=2)\n",
        "        # Low -> Low conv\n",
        "        low_to_low   = K.conv2d(low_input, self.low_to_low_kernel,\n",
        "                                strides=self.strides, padding=self.padding,\n",
        "                                data_format=\"channels_last\")\n",
        "        # Cross Add\n",
        "        high_add = high_to_high + low_to_high\n",
        "        low_add = high_to_low + low_to_low\n",
        "        return [high_add, low_add]\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        high_in_shape, low_in_shape = input_shapes\n",
        "        high_out_shape = (*high_in_shape[:3], self.high_channels)\n",
        "        low_out_shape = (*low_in_shape[:3], self.low_channels)\n",
        "        return [high_out_shape, low_out_shape]\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        out_config = {\n",
        "            **base_config,\n",
        "            \"filters\": self.filters,\n",
        "            \"alpha\": self.alpha,\n",
        "            \"filters\": self.filters,\n",
        "            \"kernel_size\": self.kernel_size,\n",
        "            \"strides\": self.strides,\n",
        "            \"padding\": self.padding,\n",
        "            \"kernel_initializer\": self.kernel_initializer,\n",
        "            \"kernel_regularizer\": self.kernel_regularizer,\n",
        "            \"kernel_constraint\": self.kernel_constraint,            \n",
        "        }\n",
        "        return out_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfD0h-ZkziYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class MixupGenerator():\n",
        "    def __init__(self, X_train, y_train, batch_size=32, alpha, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "\n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
        "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
        "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
        "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
        "        else:\n",
        "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
        "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
        "            y = y1 * y_l + y2 * (1 - y_l)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSCd6A4AK_qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, History\n",
        "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "from models import *\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "import pickle, os, time\n",
        "\n",
        "def lr_scheduler(epoch):\n",
        "    x = 0.1\n",
        "    if epoch >= 100: x /= 5.0\n",
        "    if epoch >= 150: x /= 5.0\n",
        "    if epoch >= 200: x /= 5.0\n",
        "    return x\n",
        "\n",
        "def train(alpha):\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    train_gen = ImageDataGenerator(rescale=1.0/255, horizontal_flip=True, \n",
        "                                    width_shift_range=4.0/32.0, height_shift_range=4.0/32.0,\n",
        "                                  rotation_range=40, shear_range=0.05, zoom_range=0.05)\n",
        "\n",
        "    test_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "\n",
        "    if alpha <= 0:\n",
        "        model = create_normal_wide_resnet()\n",
        "    else:\n",
        "        model = create_octconv_wide_resnet(alpha)\n",
        "    model.compile(Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), \"categorical_crossentropy\", [\"acc\"])\n",
        "    model.summary()\n",
        "\n",
        "    # convert to tpu model\n",
        "    tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "    strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "\n",
        "    batch_size = 128\n",
        "    scheduler = LearningRateScheduler(lr_scheduler)\n",
        "    hist = History()\n",
        "\n",
        "    start_time = time.time()\n",
        "    training_generator = MixupGenerator(X_train, y_train, batch_size=batch_size, alpha=1.0, datagen=train_gen)()\n",
        "    model.fit_generator(training_generator,\n",
        "                        steps_per_epoch=X_train.shape[0]//batch_size,\n",
        "                        validation_data=test_gen.flow(X_test, y_test, batch_size, shuffle=False),\n",
        "                        validation_steps=X_test.shape[0]//batch_size,\n",
        "                        callbacks=[scheduler, hist], max_queue_size=5, epochs=200)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(elapsed)\n",
        "\n",
        "    history = hist.history\n",
        "    history[\"elapsed\"] = elapsed\n",
        "\n",
        "    with open(f\"octconv_alpha_{alpha}.pkl\", \"wb\") as fp:\n",
        "        pickle.dump(history, fp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OplAXnEKL8jW",
        "colab_type": "text"
      },
      "source": [
        "# Check Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOqIPOERLRpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"octconv_alpha_0.25.pkl\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "    print(f\"Max test accuracy = {max(data['val_acc']):.04}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ypi1drP8bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"octconv_alpha_0.25.pkl\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "    model = model.load_weights(data)\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(model.history.history['loss'])), model.history.history['loss'], \n",
        "         label='Train Loss')\n",
        "plt.plot(range(len(model.history.history['loss'])), model.history.history['val_loss'], \n",
        "         label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy')\n",
        "plt.text(0.6 * epochs, 0.75 * max(model.history.history['loss']),\n",
        "         'Test loss = %.4f' %(metrics[0]))\n",
        "plt.legend(loc='best')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(model.history.history['loss'])), model.history.history['acc'], \n",
        "         label='Train Accuracy')\n",
        "plt.plot(range(len(model.history.history['loss'])), model.history.history['val_acc'], \n",
        "         label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.text(0.6 * epochs, 0.8 * max(model.history.history['acc']),\n",
        "         'Test Accuracy = %.4f' %(metrics[1]))\n",
        "plt.legend(loc='best')\n",
        "\n",
        "\n",
        "plt.savefig('training_metrics_%i.pdf' %(model_num))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIp9u63Tgc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}